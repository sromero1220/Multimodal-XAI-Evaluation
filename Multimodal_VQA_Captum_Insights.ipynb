{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Captum Insights for Visual Question Answering with Added Evaluation of Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides a simple example for the [Captum Insights API](https://captum.ai/docs/captum_insights), which is an easy to use API built on top of Captum that provides a visualization widget.\n",
    "\n",
    "\n",
    "As with the referenced tutorial, you will need the following installed on your machine:\n",
    "\n",
    "- Python Packages: torchvision, PIL, and matplotlib\n",
    "- pytorch-vqa: https://github.com/Cyanogenoid/pytorch-vqa\n",
    "- pytorch-resnet: https://github.com/Cyanogenoid/pytorch-resnet\n",
    "- A pretrained pytorch-vqa model, which can be obtained from: https://github.com/Cyanogenoid/pytorch-vqa/releases/download/v1.0/2017-08-04_00.55.19.pth\n",
    "- Create a CUDA environment with environment.yml do all dependencies and versions are correct and working\n",
    "\n",
    "Please modify the below section for your specific installation paths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "# Replace the placeholder strings with the associated \n",
    "# path for the root of pytorch-vqa and pytorch-resnet respectively\n",
    "PYTORCH_VQA_DIR = os.path.realpath(\"C:\\\\Users\\\\saroa\\\\OneDrive\\\\Documentos\\\\XAI\\\\pytorch-vqa\")\n",
    "PYTORCH_RESNET_DIR = os.path.realpath(\"C:\\\\Users\\\\saroa\\\\OneDrive\\\\Documentos\\\\XAI\\\\pytorch-resnet\")\n",
    "\n",
    "# Please modify this path to where it is located on your machine\n",
    "# you can download this model from: \n",
    "# https://github.com/Cyanogenoid/pytorch-vqa/releases/download/v1.0/2017-08-04_00.55.19.pth\n",
    "VQA_MODEL_PATH = \"models/2017-08-04_00.55.19.pth\"\n",
    "\n",
    "assert(os.path.exists(PYTORCH_VQA_DIR))\n",
    "assert(os.path.exists(PYTORCH_RESNET_DIR))\n",
    "assert(os.path.exists(VQA_MODEL_PATH))\n",
    "\n",
    "sys.path.append(PYTORCH_VQA_DIR)\n",
    "sys.path.append(PYTORCH_RESNET_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will import the necessary modules to run the code. Please make sure you have the [prerequisites to run captum](https://captum.ai/docs/getting_started), along with the pre-requisites to run this tutorial (as described in the first section)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "try:\n",
    "    import resnet  # from pytorch-resnet\n",
    "except:\n",
    "    print(\"please provide a valid path to pytorch-resnet\")\n",
    "\n",
    "try:\n",
    "    from model import Net, apply_attention, tile_2d_over_nd  # from pytorch-vqa\n",
    "    from utils import get_transform  # from pytorch-vqa\n",
    "except:\n",
    "    print(\"please provide a valid path to pytorch-vqa\")\n",
    "    \n",
    "from captum.insights import AttributionVisualizer, Batch\n",
    "from captum.insights.attr_vis.features import ImageFeature, TextFeature\n",
    "from captum.attr import TokenReferenceBase, configure_interpretable_embedding_layer, remove_interpretable_embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_on='cuda'  # change to 'cuda' if a GPU is available\n",
    "if run_on == 'cuda':\n",
    "    # Let's set the device we will use for model inference\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VQA Model Setup\n",
    "\n",
    "Let's load the VQA model (again, please refer to the [model interpretation tutorial on VQA](https://captum.ai/tutorials/Multimodal_VQA_Interpret) if you want details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_state = torch.load(VQA_MODEL_PATH, map_location=device)\n",
    "\n",
    "# reading vocabulary from saved model\n",
    "vocab = saved_state[\"vocab\"]\n",
    "\n",
    "# reading word tokens from saved model\n",
    "token_to_index = vocab[\"question\"]\n",
    "\n",
    "# reading answers from saved model\n",
    "answer_to_index = vocab[\"answer\"]\n",
    "\n",
    "num_tokens = len(token_to_index) + 1\n",
    "\n",
    "# reading answer classes from the vocabulary\n",
    "answer_words = [\"unk\"] * len(answer_to_index)\n",
    "for w, idx in answer_to_index.items():\n",
    "    answer_words[idx] = w\n",
    "    \n",
    "if run_on == 'cuda':\n",
    "    vqa_net = torch.nn.DataParallel(Net(num_tokens), device_ids=[0])\n",
    "    vqa_net.load_state_dict(saved_state[\"weights\"])\n",
    "    vqa_net = vqa_net.to(device)\n",
    "else:\n",
    "    vqa_net = Net(num_tokens)\n",
    "    state_dict = saved_state[\"weights\"]\n",
    "    from collections import OrderedDict\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        name = k[7:] if k.startswith(\"module.\") else k  # remove `module.` if it exists\n",
    "        new_state_dict[name] = v\n",
    "    vqa_net.load_state_dict(new_state_dict)\n",
    "    vqa_net = vqa_net.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    " # for visualization to convert indices to tokens for questions\n",
    "question_words = [\"unk\"] * num_tokens\n",
    "for w, idx in token_to_index.items():\n",
    "    question_words[idx] = w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's modify the VQA model to use pytorch-resnet. Our model will be called `vqa_resnet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetLayer4(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.r_model = resnet.resnet152(pretrained=True)\n",
    "        self.r_model.eval()\n",
    "        self.r_model.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.r_model.conv1(x)\n",
    "        x = self.r_model.bn1(x)\n",
    "        x = self.r_model.relu(x)\n",
    "        x = self.r_model.maxpool(x)\n",
    "        x = self.r_model.layer1(x)\n",
    "        x = self.r_model.layer2(x)\n",
    "        x = self.r_model.layer3(x)\n",
    "        return self.r_model.layer4(x)\n",
    "\n",
    "class VQA_Resnet_Model(Net):\n",
    "    def __init__(self, embedding_tokens):\n",
    "        super().__init__(embedding_tokens)\n",
    "        self.resnet_layer4 = ResNetLayer4()\n",
    "\n",
    "    def forward(self, v, q, q_len):\n",
    "        q = self.text(q, list(q_len.data))\n",
    "        v = self.resnet_layer4(v)\n",
    "\n",
    "        v = v / (v.norm(p=2, dim=1, keepdim=True).expand_as(v) + 1e-8)\n",
    "\n",
    "        a = self.attention(v, q)\n",
    "        v = apply_attention(v, a)\n",
    "\n",
    "        combined = torch.cat([v, q], dim=1)\n",
    "        answer = self.classifier(combined)\n",
    "        return answer\n",
    "    \n",
    "if run_on == 'cuda':\n",
    "    vqa_resnet = VQA_Resnet_Model(vqa_net.module.text.embedding.num_embeddings)\n",
    "    # `device_ids` contains a list of GPU ids which are used for parallelization supported by `DataParallel`\n",
    "    vqa_resnet = torch.nn.DataParallel(vqa_resnet, device_ids=[0])\n",
    "else:\n",
    "    vqa_resnet = VQA_Resnet_Model(vqa_net.text.embedding.num_embeddings)\n",
    "\n",
    "\n",
    "\n",
    "# saved vqa model's parameters\n",
    "partial_dict = vqa_net.state_dict()\n",
    "\n",
    "state = vqa_resnet.state_dict()\n",
    "state.update(partial_dict)\n",
    "vqa_resnet.load_state_dict(state)\n",
    "\n",
    "vqa_resnet.to(device)\n",
    "vqa_resnet.eval()\n",
    "\n",
    "# This is original VQA model without resnet. Removing it, since we do not need it\n",
    "del vqa_net\n",
    "\n",
    "# this is necessary for the backpropagation of RNNs models in eval mode\n",
    "torch.backends.cudnn.enabled = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Utilities\n",
    "\n",
    "Now we will need some utility functions for the inputs of our model. \n",
    "\n",
    "Let's start off with our image input transform function. We will separate out the normalization step from the transform in order to view the original image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 448  # scale image to given size and center\n",
    "central_fraction = 1.0\n",
    "\n",
    "transform = get_transform(image_size, central_fraction=central_fraction)\n",
    "transform_normalize = transform.transforms.pop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the input question, we will need an encoding function (to go from words -> indices):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_question(question):\n",
    "    \"\"\" Turn a question into a vector of indices and a question length \"\"\"\n",
    "    question_arr = question.lower().split()\n",
    "    vec = torch.zeros(len(question_arr), device=device).long()\n",
    "    for i, token in enumerate(question_arr):\n",
    "        index = token_to_index.get(token, 0)\n",
    "        vec[i] = index\n",
    "    return vec, torch.tensor(len(question_arr), device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Inputs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The insights API utilises captum's attribution API under the hood, hence we will need a baseline for our inputs. A baseline is (typically) a neutral output to reference in order for our attribution algorithm(s) to understand which features are important in making a prediction (this is very simplified explanation, 'Remark 1' in the [Integrated Gradients paper](https://arxiv.org/pdf/1703.01365.pdf) has an excellent explanation on why they must be utilised).\n",
    "\n",
    "For images and for the purpose of this tutorial, we will let this baseline be the zero vector (a black image)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_image(x):\n",
    "    return x * 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For sentences, as done in the multi-modal VQA tutorial, we will use a sentence composed of padded symbols.\n",
    "\n",
    "We will also require to pass our model through the [`configure_interpretable_embedding_layer`](https://captum.ai/api/utilities.html?highlight=configure_interpretable_embedding_layer#captum.attr._models.base.configure_interpretable_embedding_layer) function, which separates the embedding layer and precomputes word embeddings. To put it simply, this function allows us to precompute and give the embedding vectors directly to our model, which will allow us to reference the words associated to particular embeddings (for visualization purposes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\saroa\\miniconda3\\envs\\XAI\\lib\\site-packages\\captum\\attr\\_models\\base.py:192: UserWarning: In order to make embedding layers more interpretable they will be replaced with an interpretable embedding layer which wraps the original embedding layer and takes word embedding vectors as inputs of the forward function. This allows us to generate baselines for word embeddings and compute attributions for each embedding dimension. The original embedding layer must be set back by calling `remove_interpretable_embedding_layer` function after model interpretation is finished. \n",
      "  \"In order to make embedding layers more interpretable they will \"\n"
     ]
    }
   ],
   "source": [
    "if run_on == 'cuda':\n",
    "    interpretable_embedding = configure_interpretable_embedding_layer(\n",
    "        vqa_resnet, \"module.text.embedding\")\n",
    "else:\n",
    "    interpretable_embedding = configure_interpretable_embedding_layer(\n",
    "        vqa_resnet, \"text.embedding\")\n",
    "\n",
    "\n",
    "PAD_IND = token_to_index[\"pad\"]\n",
    "token_reference = TokenReferenceBase(reference_token_idx=PAD_IND)\n",
    "\n",
    "def baseline_text(x):\n",
    "    seq_len = x.size(0)\n",
    "    ref_indices = token_reference.generate_reference(seq_len, device=device).unsqueeze(\n",
    "        0\n",
    "    )\n",
    "    return interpretable_embedding.indices_to_embeddings(ref_indices).squeeze(0)\n",
    "\n",
    "def input_text_transform(x):\n",
    "    return interpretable_embedding.indices_to_embeddings(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the Insights API\n",
    "\n",
    "Finally we have reached the relevant part of the tutorial.\n",
    "\n",
    "First let's create a utility function to allow us to pass data into the insights API. This function will essentially produce `Batch` objects, which tell the insights API what your inputs, labels and any additional arguments are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vqa_dataset(image, questions, targets):\n",
    "    img = Image.open(image).convert(\"RGB\")\n",
    "    img = transform(img).unsqueeze(0).to(device)\n",
    "\n",
    "    for question, target in zip(questions, targets):\n",
    "        q, q_len = encode_question(question)\n",
    "\n",
    "        q = q.unsqueeze(0)\n",
    "        q_len = q_len.unsqueeze(0)\n",
    "\n",
    "        target_idx = answer_to_index[target]\n",
    "\n",
    "        yield Batch(\n",
    "            inputs=(img, q), labels=(target_idx,), additional_args=q_len\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create our `AttributionVisualizer`, to do this we need the following:\n",
    "\n",
    "- A score function, which tells us how to interpret the model's output vector\n",
    "- Description of the input features given to the model\n",
    "- The data to visualize (as described above)\n",
    "- Description of the output (the class names), in our case this is our answer words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case, we want to produce a single answer output via softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_func(o):\n",
    "    return F.softmax(o, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function will convert a sequence of question indices to the associated question words for visualization purposes. This will be provided to the `TextFeature` object to describe text features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def itos(input):\n",
    "    return [question_words[int(i)] for i in input.squeeze(0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define some dummy data to visualize using the function we declared earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = vqa_dataset(\"./img/vqa/siamese.jpg\", \n",
    "    [\"what is on the picture\",\n",
    "    \"what color is the cat\",\n",
    "    \"where color are the cat eyes\" ],\n",
    "    [\"cat\", \"white and brown\", \"blue\"]\n",
    ")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's describe our features. Each feature requires an input transformation function and a set of baselines. As described earlier, we will use the black image for the image baseline and a padded sequence for the text baseline.\n",
    "\n",
    "The input image will be transformed via our normalization transform (`transform_normalize`).\n",
    "Our input text will need to be transformed into embeddings, as it is a sequence of indices. Our model only accepts embeddings as input, as we modified the model with `configure_interpretable_embedding_layer` earlier.\n",
    "\n",
    "We also need to provide how the input text should be transformed in order to be visualized, which will be accomplished through the `itos` function, as described earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    ImageFeature(\n",
    "        \"Picture\",\n",
    "        input_transforms=[transform_normalize],\n",
    "        baseline_transforms=[baseline_image],\n",
    "    ),\n",
    "    TextFeature(\n",
    "        \"Question\",\n",
    "        input_transforms=[input_text_transform],\n",
    "        baseline_transforms=[baseline_text],\n",
    "        visualization_transform=itos,\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An addition to the code was made to create perturbations for both text and images, enabling the evaluation of different explanation methods on sensitivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\saroa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\saroa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "from nltk.corpus import wordnet\n",
    "from itertools import chain\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "\n",
    "def perturb_image(image, noise_level=0.1, device='cpu'):\n",
    "    if isinstance(image, Image.Image):\n",
    "        image = transforms.ToTensor()(image).to(device)\n",
    "    \n",
    "    noise = torch.randn(image.size(), device=device) * noise_level\n",
    "    perturbed_image = image + noise\n",
    "    perturbed_image = torch.clamp(perturbed_image, 0, 1)  # Ensure pixel values are within [0, 1]\n",
    "    \n",
    "    return perturbed_image\n",
    "\n",
    "def get_synonyms(word):\n",
    "    synonyms = wordnet.synsets(word)\n",
    "    return set(chain.from_iterable([word.lemma_names() for word in synonyms]))\n",
    "\n",
    "def perturb_text(text, perturbation_rate=0.1):\n",
    "\n",
    "    words = text.split()\n",
    "    num_perturb = int(len(words) * perturbation_rate)\n",
    "    indices = random.sample(range(len(words)), num_perturb)\n",
    "    \n",
    "    for i in indices:\n",
    "        synonyms = get_synonyms(words[i])\n",
    "        if synonyms:\n",
    "            words[i] = random.choice(list(synonyms))\n",
    "    \n",
    "    perturbed_text = ' '.join(words)\n",
    "    return perturbed_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, all the different functions for evaluating models on the multimodal dataset is coded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "from textstat import flesch_kincaid_grade\n",
    "import torch\n",
    "from captum.attr import IntegratedGradients\n",
    "\n",
    "\n",
    "# Evaluation functions\n",
    "def feature_importance_consistency(attributions, model_weights):\n",
    "    # Summarize attributions to match the dimensionality of model weights\n",
    "    summarized_attributions = torch.mean(torch.tensor(attributions), dim=0).detach().cpu().numpy()\n",
    "    weights = model_weights.detach().cpu().numpy()\n",
    "    return spearmanr(summarized_attributions, weights[:len(summarized_attributions)]).correlation\n",
    "\n",
    "def perturbation_test(attributions, inputs, model, additional_args, perturbation_factor=0.1):\n",
    "    perturbed_inputs = []\n",
    "    for input in inputs:\n",
    "        perturbed_input = []\n",
    "        for element in input:\n",
    "            if isinstance(element, torch.Tensor):\n",
    "                perturbed_input.append(element.clone())\n",
    "            else:\n",
    "                perturbed_input.append(element)\n",
    "        perturbed_inputs.append(perturbed_input)\n",
    "\n",
    "    # Get the device of the inputs\n",
    "    device = inputs[0][0].device\n",
    "\n",
    "    # Convert attributions to tensors and move them to the same device as inputs\n",
    "    image_attributions = torch.tensor([attr[0] for attr in attributions]).to(device)\n",
    "    text_attributions = torch.tensor([attr[1] for attr in attributions]).to(device)\n",
    "\n",
    "    # Debug prints\n",
    "    print(f\"Image attributions shape: {image_attributions.shape}\")\n",
    "    print(f\"Text attributions shape: {text_attributions.shape}\")\n",
    "\n",
    "    # Ensure image_attributions matches the shape of the image input\n",
    "    for i in range(len(perturbed_inputs)):\n",
    "        batch_size, channels, height, width = perturbed_inputs[i][0].shape\n",
    "        print(f\"Perturbed input shape: {perturbed_inputs[i][0].shape}\")\n",
    "        image_perturbation = image_attributions.view(batch_size, channels, height, width) * perturbation_factor\n",
    "        perturbed_inputs[i][0] += image_perturbation\n",
    "\n",
    "        # Ensure text_attributions matches the shape of the question input\n",
    "        sequence_length = perturbed_inputs[i][1].shape[1]\n",
    "        text_perturbation = text_attributions.view(batch_size, sequence_length) * perturbation_factor\n",
    "        perturbed_inputs[i][1] += text_perturbation\n",
    "\n",
    "    original_output = model(*inputs, additional_args)\n",
    "    perturbed_output = model(*[tuple(input) for input in perturbed_inputs], additional_args)\n",
    "    return torch.norm(original_output - perturbed_output).item()\n",
    "\n",
    "\n",
    "def readability_score(text):\n",
    "    return flesch_kincaid_grade(text)\n",
    "\n",
    "def stability_test(attributions, inputs, model, additional_args, noise_level=0.1):\n",
    "    noisy_inputs = [input + noise_level * torch.randn_like(input) for input in inputs]\n",
    "    noisy_attributions = visualizer.attribution_calculation.calculate_attribution(\n",
    "        baselines=None,\n",
    "        data=noisy_inputs,\n",
    "        additional_forward_args=additional_args,\n",
    "        label=None,\n",
    "        attribution_method_name=\"IntegratedGradients\",\n",
    "        attribution_arguments={'n_steps': 25},\n",
    "        model=model\n",
    "    )\n",
    "    return torch.norm(attributions - noisy_attributions).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define our AttributionVisualizer object with the above parameters and our `vqa_resnet` model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer = AttributionVisualizer(\n",
    "    models=[vqa_resnet],\n",
    "    score_func=score_func,\n",
    "    features=features,\n",
    "    dataset=dataset,\n",
    "    classes=answer_words,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can visualize the outputs produced by the model.\n",
    "\n",
    "Insights allows [different attribution methods](https://captum.ai/docs/algorithms) to be chosen. By default, [integrated gradients](https://captum.ai/api/integrated_gradients) is selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Debugger is active!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetch data and view Captum Insights at http://localhost:53377/\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Debugger PIN: 210-765-223\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "53377"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:53377/ (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [25/Jul/2024 13:11:56] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [25/Jul/2024 13:11:56] \"GET /static/css/main.fac91593.chunk.css HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [25/Jul/2024 13:11:56] \"GET /static/js/2.c6c4604e.chunk.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [25/Jul/2024 13:11:56] \"GET /static/js/main.835ab072.chunk.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [25/Jul/2024 13:11:56] \"GET /init HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [25/Jul/2024 13:11:56] \"GET /init HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [25/Jul/2024 13:11:56] \"GET /favicon.ico HTTP/1.1\" 404 -\n",
      "c:\\Users\\saroa\\miniconda3\\envs\\XAI\\lib\\site-packages\\captum\\attr\\_utils\\batching.py:151: UserWarning: It looks like that the attribution for a gradient-based method is\n",
      "            computed in a `torch.no_grad` block or perhaps the inputs have no\n",
      "            requires_grad.\n",
      "  requires_grad.\"\"\"\n",
      "c:\\Users\\saroa\\miniconda3\\envs\\XAI\\lib\\site-packages\\captum\\_utils\\gradient.py:59: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  \"required_grads has been set automatically.\" % index\n",
      "c:\\Users\\saroa\\miniconda3\\envs\\XAI\\lib\\site-packages\\captum\\attr\\_utils\\batching.py:151: UserWarning: It looks like that the attribution for a gradient-based method is\n",
      "            computed in a `torch.no_grad` block or perhaps the inputs have no\n",
      "            requires_grad.\n",
      "  requires_grad.\"\"\"\n",
      "c:\\Users\\saroa\\miniconda3\\envs\\XAI\\lib\\site-packages\\captum\\_utils\\gradient.py:59: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  \"required_grads has been set automatically.\" % index\n",
      "c:\\Users\\saroa\\miniconda3\\envs\\XAI\\lib\\site-packages\\captum\\attr\\_utils\\batching.py:151: UserWarning: It looks like that the attribution for a gradient-based method is\n",
      "            computed in a `torch.no_grad` block or perhaps the inputs have no\n",
      "            requires_grad.\n",
      "  requires_grad.\"\"\"\n",
      "c:\\Users\\saroa\\miniconda3\\envs\\XAI\\lib\\site-packages\\captum\\_utils\\gradient.py:59: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  \"required_grads has been set automatically.\" % index\n",
      "c:\\Users\\saroa\\miniconda3\\envs\\XAI\\lib\\site-packages\\captum\\attr\\_utils\\batching.py:151: UserWarning: It looks like that the attribution for a gradient-based method is\n",
      "            computed in a `torch.no_grad` block or perhaps the inputs have no\n",
      "            requires_grad.\n",
      "  requires_grad.\"\"\"\n",
      "c:\\Users\\saroa\\miniconda3\\envs\\XAI\\lib\\site-packages\\captum\\_utils\\gradient.py:59: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  \"required_grads has been set automatically.\" % index\n",
      "127.0.0.1 - - [25/Jul/2024 13:12:12] \"POST /fetch HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "visualizer.serve(debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the visualizer is being displayed on localhost, when a explanation is run we have the requiered information to create evaluations for this models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saliency\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\saroa\\miniconda3\\envs\\XAI\\lib\\site-packages\\captum\\_utils\\gradient.py:59: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  \"required_grads has been set automatically.\" % index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------+---------------------+------------------+-------------------+--------------------------+-------------------------------------------+-------------------------------------------+---------------------------+\n",
      "|    |   noise_level |   perturbation_rate | original_label   | perturbed_label   | prediction_consistency   | original_net_contrib                      | perturbed_net_contrib                     | comparison                |\n",
      "|----+---------------+---------------------+------------------+-------------------+--------------------------+-------------------------------------------+-------------------------------------------+---------------------------|\n",
      "|  0 |           0.1 |                 0.1 | cat              | cat               | True                     | [0.8570834398269653, 0.14291661977767944] | [0.8150659799575806, 0.18493403494358063] | [ 0.04201746 -0.04201742] |\n",
      "|  1 |           0.3 |                 0.2 | cat              | cat               | True                     | [0.8570834398269653, 0.14291661977767944] | [0.7674914002418518, 0.2325086146593094]  | [ 0.08959204 -0.08959199] |\n",
      "|  2 |           0.5 |                 0.3 | cat              | cat               | True                     | [0.8570834398269653, 0.14291661977767944] | [0.7627019286155701, 0.23729805648326874] | [ 0.09438151 -0.09438144] |\n",
      "|  3 |           0.7 |                 0.4 | cat              | nothing           | False                    | [0.8570834398269653, 0.14291661977767944] | [0.496565043926239, 0.5034348964691162]   | [ 0.3605184  -0.36051828] |\n",
      "|  4 |           0.1 |                 0.1 | white and brown  | white             | False                    | [0.9194059371948242, 0.08059407770633698] | [0.8509690165519714, 0.14903093874454498] | [ 0.06843692 -0.06843686] |\n",
      "|  5 |           0.1 |                 0.1 | blue             | blue              | True                     | [0.7437815070152283, 0.2562185227870941]  | [0.6360167860984802, 0.36398324370384216] | [ 0.10776472 -0.10776472] |\n",
      "|  6 |           0.3 |                 0.2 | blue             | black             | False                    | [0.7437815070152283, 0.2562185227870941]  | [0.494953989982605, 0.505046010017395]    | [ 0.24882752 -0.24882749] |\n",
      "+----+---------------+---------------------+------------------+-------------------+--------------------------+-------------------------------------------+-------------------------------------------+---------------------------+\n"
     ]
    }
   ],
   "source": [
    "from captum.attr import (Deconvolution,DeepLift,FeatureAblation,GuidedBackprop,InputXGradient,IntegratedGradients,Occlusion,Saliency,)\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "\n",
    "SUPPORTED_ATTRIBUTION_METHODS = [Deconvolution,DeepLift,GuidedBackprop,InputXGradient,IntegratedGradients,Saliency,FeatureAblation,Occlusion,]\n",
    "ATTRIBUTION_NAMES_TO_METHODS = {\n",
    "    cls.get_name(): cls  # type: ignore\n",
    "    for cls in SUPPORTED_ATTRIBUTION_METHODS\n",
    "}\n",
    "\n",
    "def calculate_attributions(model, inputs, target, baselines, additional_args, xai_model, selected_arguments):\n",
    "    xai = ATTRIBUTION_NAMES_TO_METHODS[xai_model](model)\n",
    "    if xai_model in ['IntegratedGradients', 'FeatureAblation', 'Occlusion']:\n",
    "        attributions = xai.attribute.__wrapped__(xai, inputs=inputs, additional_forward_args=additional_args, target=target, **selected_arguments)\n",
    "    else:\n",
    "        attributions = xai.attribute.__wrapped__(xai, inputs=inputs, additional_forward_args=additional_args, target=target)\n",
    "    return attributions\n",
    "\n",
    "modality_attributions = visualizer.get_attributions()\n",
    " \n",
    "selected_arguments= visualizer.get_insights_config()['selected_arguments']    \n",
    "xai_model = visualizer.get_insights_config()['selected_method']\n",
    "print(xai_model) \n",
    "\n",
    "    \n",
    "dataset = vqa_dataset(\"./img/vqa/siamese.jpg\", \n",
    "    [\"what is on the picture\",\n",
    "    \"what color is the cat\",\n",
    "    \"where color are the cat eyes\" ],\n",
    "    [\"cat\", \"white and brown\", \"blue\"]\n",
    ")    \n",
    "\n",
    "results = []\n",
    "\n",
    "for batch in dataset:\n",
    "    original_inputs = batch.inputs\n",
    "    original_additional_args = batch.additional_args\n",
    "    target = batch.labels\n",
    "    \n",
    "    noise_level = 0.1\n",
    "    perturbation_rate = 0.1\n",
    "    \n",
    "    # Calculate original attributions\n",
    "    (original_predicted_scores, original_baselines, orginal_transformed_inputs,) = visualizer.attribution_calculation.calculate_predicted_scores(original_inputs, original_additional_args, vqa_resnet)\n",
    "    original_attributions = calculate_attributions(vqa_resnet, orginal_transformed_inputs, target, None, original_additional_args, xai_model, selected_arguments)\n",
    "    original_net_contrib = visualizer.attribution_calculation.calculate_net_contrib(original_attributions)\n",
    "\n",
    "    # Clear unused variables and cache to free GPU memory\n",
    "    del original_baselines, orginal_transformed_inputs, original_attributions\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    original_label = original_predicted_scores[0].label\n",
    "    \n",
    "    while True:\n",
    "        # Perturb image\n",
    "        perturbed_image = perturb_image(original_inputs[0][0], noise_level=noise_level, device=run_on)\n",
    "        perturbed_image = perturbed_image.unsqueeze(0).to(device)\n",
    "\n",
    "        # Perturb text\n",
    "        original_question = ' '.join(itos(original_inputs[1][0]))\n",
    "        perturbed_question = perturb_text(original_question, perturbation_rate=perturbation_rate)\n",
    "        perturbed_question_vec, perturbed_question_len = encode_question(perturbed_question)\n",
    "        perturbed_question_vec = perturbed_question_vec.unsqueeze(0)\n",
    "        perturbed_question_len = perturbed_question_len.unsqueeze(0)\n",
    "\n",
    "        perturbed_inputs = (perturbed_image, perturbed_question_vec)\n",
    "        perturbed_additional_args = perturbed_question_len\n",
    "\n",
    "\n",
    "        # Calculate new attributions with perturbed inputs\n",
    "        (perturbed_rpredicted_scores, perturbed_baselines, perturbed_transformed_inputs,) = visualizer.attribution_calculation.calculate_predicted_scores(perturbed_inputs, perturbed_additional_args, vqa_resnet)\n",
    "        perturbed_attributions = calculate_attributions(vqa_resnet, perturbed_transformed_inputs, target, None, perturbed_additional_args, xai_model, selected_arguments)\n",
    "        perturbed_net_contrib = visualizer.attribution_calculation.calculate_net_contrib(perturbed_attributions)\n",
    "\n",
    "        # Clear unused variables and cache to free GPU memory\n",
    "        del perturbed_baselines, perturbed_transformed_inputs, perturbed_attributions\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Compare original and perturbed attributions\n",
    "        perturbed_label = perturbed_rpredicted_scores[0].label\n",
    "        prediction_consistency = original_label == perturbed_label\n",
    "        comparison = np.array(original_net_contrib) - np.array(perturbed_net_contrib)\n",
    "\n",
    "        # Store the results\n",
    "        results.append({\n",
    "            'noise_level': noise_level,\n",
    "            'perturbation_rate': perturbation_rate,\n",
    "            'original_label': original_label,\n",
    "            'perturbed_label': perturbed_label,\n",
    "            'prediction_consistency': prediction_consistency,\n",
    "            'original_net_contrib': str(original_net_contrib),\n",
    "            'perturbed_net_contrib': str(perturbed_net_contrib),\n",
    "            'comparison': str(comparison)\n",
    "        })\n",
    "\n",
    "        # Check for consistency\n",
    "        if not prediction_consistency:\n",
    "            break\n",
    "        else:\n",
    "            noise_level += 0.2\n",
    "            perturbation_rate += 0.1\n",
    "\n",
    "# Create a DataFrame from the results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print(tabulate(results_df, headers='keys', tablefmt='psql'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate explanations\n",
    "model_weights = vqa_resnet.module.classifier.lin2.weight # Example for accessing model weights\n",
    "faithfulness_score = feature_importance_consistency(modality_attributions, model_weights)\n",
    "#perturbation_score = perturbation_test(attributions, inputs, vqa_resnet, additional_args)\n",
    "readability = readability_score(\"Generated Explanation Text\")  # Replace with actual text\n",
    "#sensitivity_score = stability_test(attributions, inputs, vqa_resnet)\n",
    "\n",
    "# Print or log evaluation results\n",
    "print(f\"Faithfulness Score: {faithfulness_score}\")\n",
    "#print(f\"Perturbation Score: {perturbation_score}\")\n",
    "print(f\"Readability Score: {readability}\")\n",
    "#print(f\"Sensitivity Score: {sensitivity_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show a screenshot if using notebook non-interactively\n",
    "import IPython.display\n",
    "IPython.display.Image(filename='img/captum_insights_vqa.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, since we are done with visualization, we will revert the change to the model we made with `configure_interpretable_embedding_layer`. To do this, we will invoke the `remove_interpretable_embedding_layer` function. Uncomment the line below to execute the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove_interpretable_embedding_layer(vqa_resnet, interpretable_embedding)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
